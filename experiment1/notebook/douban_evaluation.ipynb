{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "class StubLogger(object):\n",
    "    def __getattr__(self, name):\n",
    "        return self.log_print\n",
    "\n",
    "    def log_print(self, msg, *args):\n",
    "        print(msg % args)\n",
    "\n",
    "LOGGER = StubLogger()\n",
    "LOGGER.info(\"Hello %s!\", \"world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import time\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def elapsed_timer(message):\n",
    "    start_time = time.time()\n",
    "    yield\n",
    "    LOGGER.info(message.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hitsAtK(user_ranks, k=10):\n",
    "    return (user_ranks[\"rank\"].notna() & (user_ranks[\"rank\"] <= k)).sum(skipna=True)\n",
    "\n",
    "\n",
    "def precisionAtK(user_ranks, k):\n",
    "    precision = float(hitsAtK(user_ranks, k)) / k\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recallAtK(user_ranks, k):\n",
    "    recall = float(hitsAtK(user_ranks, k)) / user_ranks.shape[0]\n",
    "    return recall\n",
    "\n",
    "\n",
    "def averagePrecisionAtK(user_ranks, k):\n",
    "    user_ranks = user_ranks.sort_values(by=\"rank\")\n",
    "    k_user_ranks = user_ranks.head(min(user_ranks.shape[0], k))\n",
    "    k_correct_user_ranks = k_user_ranks[k_user_ranks[\"rank\"].notna() & (k_user_ranks[\"rank\"] <= k)]\n",
    "\n",
    "    if k_correct_user_ranks.shape[0] > 0:\n",
    "        #print k_correct_user_ranks\n",
    "        score = 0.0\n",
    "        for row in range(k_correct_user_ranks.shape[0]):\n",
    "            tmp_user_ranks = k_correct_user_ranks.head(row + 1)\n",
    "            row_k = k_correct_user_ranks.iloc[row][\"rank\"]\n",
    "            score = score + precisionAtK(tmp_user_ranks, row_k)\n",
    "        avgPrec = float(score) / min(user_ranks.shape[0], k)\n",
    "    else:\n",
    "        avgPrec = 0.0\n",
    "    return avgPrec\n",
    "\n",
    "\n",
    "def ndcgAtK(user_ranks, k):\n",
    "    def dcg(rank):\n",
    "        return 1.0 / np.log2(rank + 1)\n",
    "\n",
    "\n",
    "    user_ranks = user_ranks.sort_values(by=\"rank\")\n",
    "    k_user_ranks = user_ranks.head(min(user_ranks.shape[0], k))\n",
    "    k_ranks = k_user_ranks[k_user_ranks[\"rank\"].notna() & (k_user_ranks[\"rank\"] <= k)][\"rank\"].values\n",
    "\n",
    "    ranks_idcg = dcg(np.arange(1, k_user_ranks.shape[0] + 1))\n",
    "    ranks_dcg = dcg(k_ranks)\n",
    "\n",
    "    ndcg = float(np.sum(ranks_dcg)) / np.sum(ranks_idcg)\n",
    "    return ndcg\n",
    "\n",
    "\n",
    "def topEventsAtK(user_ranks, k):\n",
    "    user_ranks = user_ranks.sort_values(by=\"rank\")\n",
    "    k_user_ranks = user_ranks.head(min(user_ranks.shape[0], k))\n",
    "    return pd.DataFrame({\n",
    "        \"event_id\": k_user_ranks[\"event_id\"].drop_duplicates().sort_values()\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quickMetricsPerUser(user_ranks):\n",
    "    return pd.DataFrame({\n",
    "        \"precision_at_10\": [precisionAtK(user_ranks, 10)],\n",
    "        \"recall_at_10\": [recallAtK(user_ranks, 10)],\n",
    "        \"ndcg_at_10\": [ndcgAtK(user_ranks, 10)],\n",
    "    })\n",
    "\n",
    "\n",
    "def basicMetricsPerUser(user_ranks):\n",
    "    return pd.DataFrame({\n",
    "        \"precision_at_10\": [precisionAtK(user_ranks, 10)],\n",
    "        \"recall_at_10\": [recallAtK(user_ranks, 10)],\n",
    "        \"ndcg_at_50\": [ndcgAtK(user_ranks, 50)],\n",
    "        \"ndcg_at_20\": [ndcgAtK(user_ranks, 20)],\n",
    "        \"ndcg_at_10\": [ndcgAtK(user_ranks, 10)],\n",
    "        \"avg_prec_at_20\": [averagePrecisionAtK(user_ranks, 20)],\n",
    "        \"avg_prec_at_10\": [averagePrecisionAtK(user_ranks, 10)],\n",
    "    })\n",
    "\n",
    "\n",
    "def quadMetricsPerUser(user_ranks):\n",
    "    return pd.DataFrame({\n",
    "        \"precision_at_50\": [precisionAtK(user_ranks, 50)],\n",
    "        \"precision_at_20\": [precisionAtK(user_ranks, 20)],\n",
    "        \"precision_at_10\": [precisionAtK(user_ranks, 10)],\n",
    "        \"precision_at_5\": [precisionAtK(user_ranks, 5)],\n",
    "        \"recall_at_50\": [recallAtK(user_ranks, 50)],\n",
    "        \"recall_at_20\": [recallAtK(user_ranks, 20)],\n",
    "        \"recall_at_10\": [recallAtK(user_ranks, 10)],\n",
    "        \"recall_at_5\": [recallAtK(user_ranks, 5)],\n",
    "        \"ndcg_at_50\": [ndcgAtK(user_ranks, 50)],\n",
    "        \"ndcg_at_20\": [ndcgAtK(user_ranks, 20)],\n",
    "        \"ndcg_at_10\": [ndcgAtK(user_ranks, 10)],\n",
    "        \"ndcg_at_5\": [ndcgAtK(user_ranks, 5)],\n",
    "        \"avg_prec_at_20\": [averagePrecisionAtK(user_ranks, 20)],\n",
    "        \"avg_prec_at_10\": [averagePrecisionAtK(user_ranks, 10)],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalMetrics(rank_data, *, runMetricsPerUser=None):\n",
    "    if runMetricsPerUser is None:\n",
    "        runMetricsPerUser = basicMetricsPerUser\n",
    "\n",
    "    user_metrics = rank_data.groupby(\"user_id\").apply(runMetricsPerUser)\n",
    "    top_events_at_10 = rank_data.groupby(\"user_id\").apply(topEventsAtK, 10)\n",
    "    top_events_at_20 = rank_data.groupby(\"user_id\").apply(topEventsAtK, 20)\n",
    "\n",
    "    metrics = dict()\n",
    "    # Precision\n",
    "    if \"precision_at_50\" in user_metrics.columns:\n",
    "        precision_50 = user_metrics[\"precision_at_50\"].sum(skipna=True) / user_metrics.shape[0]\n",
    "        metrics[\"precision_at_50\"] = [precision_50]\n",
    "    if \"precision_at_20\" in user_metrics.columns:\n",
    "        precision_20 = user_metrics[\"precision_at_20\"].sum(skipna=True) / user_metrics.shape[0]\n",
    "        metrics[\"precision_at_20\"] = [precision_20]\n",
    "    if \"precision_at_10\" in user_metrics.columns:\n",
    "        precision_10 = user_metrics[\"precision_at_10\"].sum(skipna=True) / user_metrics.shape[0]\n",
    "        metrics[\"precision_at_10\"] = [precision_10]\n",
    "    if \"precision_at_5\" in user_metrics.columns:\n",
    "        precision_5 = user_metrics[\"precision_at_5\"].sum(skipna=True) / user_metrics.shape[0]\n",
    "        metrics[\"precision_at_5\"] = [precision_5]\n",
    "\n",
    "    # Recall\n",
    "    if \"recall_at_50\" in user_metrics.columns:\n",
    "        recall_50 = user_metrics[\"recall_at_50\"].sum(skipna=True) / user_metrics.shape[0]\n",
    "        metrics[\"recall_at_50\"] = [recall_50]\n",
    "    if \"recall_at_20\" in user_metrics.columns:\n",
    "        recall_20 = user_metrics[\"recall_at_20\"].sum(skipna=True) / user_metrics.shape[0]\n",
    "        metrics[\"recall_at_20\"] = [recall_20]\n",
    "    if \"recall_at_10\" in user_metrics.columns:\n",
    "        recall_10 = user_metrics[\"recall_at_10\"].sum(skipna=True) / user_metrics.shape[0]\n",
    "        metrics[\"recall_at_10\"] = [recall_10]\n",
    "    if \"recall_at_5\" in user_metrics.columns:\n",
    "        recall_5 = user_metrics[\"recall_at_5\"].sum(skipna=True) / user_metrics.shape[0]\n",
    "        metrics[\"recall_at_5\"] = [recall_5]\n",
    "\n",
    "    # F1 Score\n",
    "    if \"precision_at_50\" in user_metrics.columns and \"recall_at_50\" in user_metrics.columns:\n",
    "        if precision_50 + recall_50 > 0:\n",
    "            f1_score_50 =  2 * ((precision_50 * recall_50) / (precision_50 + recall_50))\n",
    "        else:\n",
    "            f1_score_50 = 0.0\n",
    "        metrics[\"f1_score_at_50\"] = [f1_score_50]\n",
    "    if \"precision_at_20\" in user_metrics.columns and \"recall_at_20\" in user_metrics.columns:\n",
    "        if precision_20 + recall_20 > 0:\n",
    "            f1_score_20 =  2 * ((precision_20 * recall_20) / (precision_20 + recall_20))\n",
    "        else:\n",
    "            f1_score_20 = 0.0\n",
    "        metrics[\"f1_score_at_20\"] = [f1_score_20]\n",
    "    if \"precision_at_10\" in user_metrics.columns and \"recall_at_10\" in user_metrics.columns:\n",
    "        if precision_10 + recall_10 > 0:\n",
    "            f1_score_10 =  2 * ((precision_10 * recall_10) / (precision_10 + recall_10))\n",
    "        else:\n",
    "            f1_score_10 = 0.0\n",
    "        metrics[\"f1_score_at_10\"] = [f1_score_10]\n",
    "    if \"precision_at_5\" in user_metrics.columns and \"recall_at_5\" in user_metrics.columns:\n",
    "        if precision_5 + recall_5 > 0:\n",
    "            f1_score_5 =  2 * ((precision_5 * recall_5) / (precision_5 + recall_5))\n",
    "        else:\n",
    "            f1_score_5 = 0.0\n",
    "        metrics[\"f1_score_at_5\"] = [f1_score_5]\n",
    "\n",
    "    # NDCG@50\n",
    "    if \"ndcg_at_50\" in user_metrics.columns:\n",
    "        ndcg_50 = user_metrics[\"ndcg_at_50\"].sum(skipna=True) / user_metrics.shape[0]\n",
    "        metrics[\"ndcg_at_50\"] = [ndcg_50]\n",
    "    # NDCG@20\n",
    "    if \"ndcg_at_20\" in user_metrics.columns:\n",
    "        ndcg_20 = user_metrics[\"ndcg_at_20\"].sum(skipna=True) / user_metrics.shape[0]\n",
    "        metrics[\"ndcg_at_20\"] = [ndcg_20]\n",
    "    # NDCG@10\n",
    "    if \"ndcg_at_10\" in user_metrics.columns:\n",
    "        ndcg_10 = user_metrics[\"ndcg_at_10\"].sum(skipna=True) / user_metrics.shape[0]\n",
    "        metrics[\"ndcg_at_10\"] = [ndcg_10]\n",
    "    \n",
    "    if \"ndcg_at_5\" in user_metrics.columns:\n",
    "        ndcg_5 = user_metrics[\"ndcg_at_5\"].sum(skipna=True) / user_metrics.shape[0]\n",
    "        metrics[\"ndcg_at_5\"] = [ndcg_5]\n",
    "    # MAP@20\n",
    "    if \"avg_prec_at_20\" in user_metrics.columns:\n",
    "        map_at_20 = user_metrics[\"avg_prec_at_20\"].sum(skipna=True) / user_metrics.shape[0]\n",
    "        metrics[\"map_at_20\"] = [map_at_20]\n",
    "    # MAP@10\n",
    "    if \"avg_prec_at_10\" in user_metrics.columns:\n",
    "        map_at_10 = user_metrics[\"avg_prec_at_10\"].sum(skipna=True) / user_metrics.shape[0]\n",
    "        metrics[\"map_at_10\"] = [map_at_10]\n",
    "\n",
    "    # Mean Rank (calculated only over the rank_data without NA's)\n",
    "    mean_ranks = rank_data[\"rank\"].mean(skipna=True)\n",
    "    if np.isnan(mean_ranks):\n",
    "        mean_ranks = 0.0\n",
    "    # User Coverage (it cannot be calculated here)\n",
    "    user_coverage = 0.0\n",
    "    # Event Coverage\n",
    "    event_coverage_10 = float(top_events_at_10.drop_duplicates().shape[0]) / rank_data[\"event_id\"].drop_duplicates().shape[0]\n",
    "    event_coverage_20 = float(top_events_at_20.drop_duplicates().shape[0]) / rank_data[\"event_id\"].drop_duplicates().shape[0]\n",
    "\n",
    "    metrics[\"mean_ranks\"] = [mean_ranks]\n",
    "    metrics[\"user_coverage\"] = [user_coverage]\n",
    "    metrics[\"event_coverage_at_10\"] = [event_coverage_10]\n",
    "    metrics[\"event_coverage_at_20\"] = [event_coverage_20]\n",
    "    # This percentage of NA's only affects the mean_ranks metric,\n",
    "    # the other metrics consider the NA's in the calculation\n",
    "    metrics[\"perc_user_events_rank_NA\"] = float(rank_data[\"rank\"].isna().sum()) / rank_data.shape[0]\n",
    "    \n",
    "    return pd.DataFrame(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def definePastRSVPGroups(count_data, new_col_name):\n",
    "    count_data[\"past_rsvps\"] = np.nan\n",
    "    freqs = count_data[\"freq\"]\n",
    "\n",
    "    count_data.loc[freqs <= 0, \"past_rsvps\"] = \"0\"\n",
    "    count_data.loc[freqs <= 1, \"past_rsvps\"] = \"1\"\n",
    "    count_data.loc[freqs <= 2, \"past_rsvps\"] = \"2\"\n",
    "    count_data.loc[freqs <= 3, \"past_rsvps\"] = \"3\"\n",
    "    count_data.loc[freqs <= 4, \"past_rsvps\"] = \"4\"\n",
    "    count_data.loc[freqs <= 5, \"past_rsvps\"] = \"5\"\n",
    "    count_data.loc[(6 <= freqs) & (freqs <= 10), \"past_rsvps\"] = \"6-10\"\n",
    "    count_data.loc[(11 <= freqs) & (freqs <= 20), \"past_rsvps\"] = \"11-20\"\n",
    "    count_data.loc[freqs > 20, \"past_rsvps\"] = \">20\"\n",
    "\n",
    "    count_data[\"past_rsvps\"] = count_data[\"past_rsvps\"].astype(\"category\")\n",
    "    count_data = count_data.rename(columns={\"past_rsvps\": new_col_name})\n",
    "\n",
    "    return count_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluatePartition(object):\n",
    "    def __init__(self, partition_dir, *, \n",
    "                 runMetricsPerUser=None):\n",
    "        self.partition_dir = partition_dir\n",
    "        self.runMetricsPerUser = runMetricsPerUser\n",
    "        if self.runMetricsPerUser is None:\n",
    "            self.runMetricsPerUser = basicMetricsPerUser\n",
    "\n",
    "        self.user_event_rsvp_test_filepath = os.path.join(self.partition_dir, \"user-event-rsvp_test.tsv\")\n",
    "#         self.count_events_per_test_user_filepath = os.path.join(self.partition_dir, \"count_events_per_test-user_train.tsv\")\n",
    "#         self.count_users_per_test_event_filepath = os.path.join(self.partition_dir, \"count_users_per_test-event_train.tsv\")\n",
    "        \n",
    "        self.user_event_rsvps_test = pd.read_csv(self.user_event_rsvp_test_filepath,\n",
    "                                                 sep='\\t', header=None, names=[\"user_id\", \"event_id\"])\n",
    "#         self.count_events_per_test_user = pd.read_csv(self.count_events_per_test_user_filepath,\n",
    "#                                       sep='\\t', header=None, names=[\"user_id\", \"freq\"])\n",
    "#         self.count_users_per_test_event = pd.read_csv(self.count_users_per_test_event_filepath,\n",
    "#                                        sep='\\t', header=None, names=[\"event_id\", \"freq\"])\n",
    "\n",
    "        self.map_user_event_rsvps_test = self._read_map_user_events_test(self.user_event_rsvps_test)\n",
    "#         self.user_count = definePastRSVPGroups(self.count_events_per_test_user, \"user_past_rsvps\")\n",
    "#         self.event_count = definePastRSVPGroups(self.count_users_per_test_event, \"event_past_rsvps\")\n",
    "        \n",
    "        print(\"users: {0}, events: {1}\".format(self.user_event_rsvps_test[\"user_id\"].drop_duplicates().count(),\n",
    "                                               self.user_event_rsvps_test[\"event_id\"].drop_duplicates().count()))\n",
    "        \n",
    "        self.group_evaluations = {\n",
    "            \"partition\": None,\n",
    "            \"partition-user\": None,\n",
    "            \"partition-event\": None,\n",
    "            \"partition-user-event\": None\n",
    "        }\n",
    "\n",
    "    def _read_map_user_events_test(self, user_event_rsvps):\n",
    "        user_events = dict()\n",
    "        for row in user_event_rsvps.itertuples():\n",
    "            user_events.setdefault(row.user_id, set()).add(row.event_id)\n",
    "        return user_events\n",
    "\n",
    "    def add_evaluation(self, recommendation_filepath, group_vars, *,\n",
    "                       partition, algorithm, model_params,\n",
    "                       pre_process_user_id=None, pre_process_event_id=None,\n",
    "                       runMetricsPerUser=None):\n",
    "        if runMetricsPerUser is None:\n",
    "            runMetricsPerUser = self.runMetricsPerUser\n",
    "\n",
    "        relevant_ranks = self._select_relevant_ranks(recommendation_filepath,\n",
    "                                                     pre_process_user_id=pre_process_user_id,\n",
    "                                                     pre_process_event_id=pre_process_event_id)\n",
    "        self._evaluate_ranked_data(relevant_ranks, group_vars,\n",
    "                                   partition=partition, algorithm=algorithm, model_params=model_params,\n",
    "                                   runMetricsPerUser=runMetricsPerUser)\n",
    "\n",
    "    def _select_relevant_ranks(self, recommendation_filepath, *,\n",
    "                               pre_process_user_id=None, pre_process_event_id=None):\n",
    "        if pre_process_user_id is None:\n",
    "            pre_process_user_id = lambda x: x\n",
    "        if pre_process_event_id is None:\n",
    "            pre_process_event_id = lambda x: x\n",
    "\n",
    "        relevant_ranked_user_ids = list()\n",
    "        relevant_ranked_event_ids = list()\n",
    "        relevant_ranks = list()\n",
    "\n",
    "        recommendations = pd.read_csv(recommendation_filepath, sep='\\t',\n",
    "                                      header=None, names=[\"user_id\", \"recommendation\"])\n",
    "        recommendations[\"user_id\"] = pre_process_user_id(recommendations[\"user_id\"])\n",
    "\n",
    "        for row in recommendations.itertuples():\n",
    "            # Get the relevant events per user\n",
    "            new_event_ids_test = self.map_user_event_rsvps_test[row.user_id]\n",
    "            ranked_events = set()\n",
    "\n",
    "            # Check if the model was capable of predicting a ranked list or not\n",
    "            #   There is a predicted value different from ''\n",
    "            if len(row) > 1 and row.recommendation:\n",
    "                ranked_event_list = row.recommendation.split(',')\n",
    "                # Find the relevant events (from new_event_ids_test) in the ranked recommended list and get its ranks\n",
    "                for i, recommendation_str in enumerate(ranked_event_list):\n",
    "                    if ranked_event_list[i]:\n",
    "                        # Separate the new_event_id from the predicted score (use only the 1st one)\n",
    "                        recommendation = recommendation_str.split(':')\n",
    "                        new_event_id = pre_process_event_id(recommendation[0])\n",
    "                        if new_event_id in new_event_ids_test:\n",
    "                            relevant_ranked_user_ids.append(row.user_id)\n",
    "                            relevant_ranked_event_ids.append(new_event_id)\n",
    "                            relevant_ranks.append(i + 1)\n",
    "                            ranked_events.add(new_event_id)\n",
    "\n",
    "            # IDEA: If the Model was not capable of recommeding this event to the user we consider a NA rank\n",
    "            #   * Therefore, we consider ranking larger that limit (e.g. 100) the same as didn't ranking any event to the user\n",
    "            for relevant_event in new_event_ids_test:\n",
    "                if relevant_event not in ranked_events:\n",
    "                    relevant_ranked_user_ids.append(row.user_id)\n",
    "                    relevant_ranked_event_ids.append(relevant_event)\n",
    "                    relevant_ranks.append(np.nan)\n",
    "\n",
    "        return pd.DataFrame({\n",
    "            \"user_id\": relevant_ranked_user_ids,\n",
    "            \"event_id\": relevant_ranked_event_ids,\n",
    "            \"rank\": relevant_ranks\n",
    "        })\n",
    "\n",
    "    def _evaluate_ranked_data(self, relevant_ranks, group_vars, *,\n",
    "                              partition, algorithm, model_params, runMetricsPerUser):\n",
    "        rank_data = relevant_ranks\n",
    "\n",
    "#         rank_data = pd.merge(rank_data, self.user_count[[\"user_id\", \"user_past_rsvps\"]], on=\"user_id\")\n",
    "#         rank_data = pd.merge(rank_data, self.event_count[[\"event_id\", \"event_past_rsvps\"]], on=\"event_id\")\n",
    "#         rank_data = rank_data.sort_values(by=[\"user_past_rsvps\", \"event_past_rsvps\"])\n",
    "\n",
    "        if group_vars == \"partition\":\n",
    "            eval_rank_data = evalMetrics(rank_data, runMetricsPerUser=runMetricsPerUser)\n",
    "#         elif group_vars == \"partition-user\":\n",
    "#             eval_rank_data = rank_data.groupby(\"user_past_rsvps\").apply(evalMetrics, runMetricsPerUser=runMetricsPerUser)\n",
    "#         elif group_vars == \"partition-event\":\n",
    "#             eval_rank_data = rank_data.groupby(\"event_past_rsvps\").apply(evalMetrics, runMetricsPerUser=runMetricsPerUser)\n",
    "#         elif group_vars == \"partition-user-event\":\n",
    "#             eval_rank_data = rank_data.groupby([\"user_past_rsvps\", \"event_past_rsvps\"]).apply(evalMetrics, runMetricsPerUser=runMetricsPerUser)\n",
    "\n",
    "        # Add the partition name\n",
    "        eval_rank_data[\"partition\"] = partition\n",
    "\n",
    "        eval_rank_data[\"algorithm\"] = algorithm\n",
    "\n",
    "        if model_params:\n",
    "            eval_rank_data[\"model_params\"] = model_params\n",
    "        else:\n",
    "            eval_rank_data[\"model_params\"] = np.nan\n",
    "\n",
    "        if self.group_evaluations[group_vars] is None:\n",
    "            self.group_evaluations[group_vars] = pd.DataFrame()\n",
    "        self.group_evaluations[group_vars] = pd.concat([self.group_evaluations[group_vars],\n",
    "                                                        eval_rank_data], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Douban dataset path combiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def douban_file(*args):\n",
    "    return os.path.join(\"../data/douban\", *args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users: 4600, events: 2471\n"
     ]
    }
   ],
   "source": [
    "douban = EvaluatePartition(\"../data/douban\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- 95.06542897224426s - deepwalk\n",
      "-- 94.58054971694946s - category2vec\n"
     ]
    }
   ],
   "source": [
    "# with elapsed_timer(\"{0}s - %s\" % (result_dir,)):\n",
    "with elapsed_timer(\"-- {0}s - %s\" % (\"deepwalk\",)):\n",
    "    douban.add_evaluation(douban_file(\"deepwalk.tsv\"), \"partition\", partition=1, algorithm=\"deepwalk\", model_params=\"deepwalk\")\n",
    "with elapsed_timer(\"-- {0}s - %s\" % (\"category2vec\",)):\n",
    "    douban.add_evaluation(douban_file(\"category2vec.tsv\"), \"partition\", partition=1, algorithm=\"category2vec\", model_params=\"category2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision_at_10</th>\n",
       "      <th>recall_at_10</th>\n",
       "      <th>f1_score_at_10</th>\n",
       "      <th>ndcg_at_50</th>\n",
       "      <th>ndcg_at_20</th>\n",
       "      <th>ndcg_at_10</th>\n",
       "      <th>map_at_20</th>\n",
       "      <th>map_at_10</th>\n",
       "      <th>mean_ranks</th>\n",
       "      <th>user_coverage</th>\n",
       "      <th>event_coverage_at_10</th>\n",
       "      <th>event_coverage_at_20</th>\n",
       "      <th>perc_user_events_rank_NA</th>\n",
       "      <th>partition</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>model_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>deepwalk</td>\n",
       "      <td>deepwalk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>category2vec</td>\n",
       "      <td>category2vec</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision_at_10  recall_at_10  f1_score_at_10  ndcg_at_50  ndcg_at_20  \\\n",
       "0              0.0           0.0             0.0         0.0         0.0   \n",
       "1              0.0           0.0             0.0         0.0         0.0   \n",
       "\n",
       "   ndcg_at_10  map_at_20  map_at_10  mean_ranks  user_coverage  \\\n",
       "0         0.0        0.0        0.0         0.0            0.0   \n",
       "1         0.0        0.0        0.0         0.0            0.0   \n",
       "\n",
       "   event_coverage_at_10  event_coverage_at_20  perc_user_events_rank_NA  \\\n",
       "0                   1.0                   1.0                       1.0   \n",
       "1                   1.0                   1.0                       1.0   \n",
       "\n",
       "   partition     algorithm  model_params  \n",
       "0          1      deepwalk      deepwalk  \n",
       "1          1  category2vec  category2vec  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "douban.group_evaluations[\"partition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
